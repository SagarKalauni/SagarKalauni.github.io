---
title: 'Stat 562 HW #4'
author: "Zahra"
date: "2023-12-09"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

1. Consider a neural network with two hidden layers: p = 4 input units, 2 units in the first hidden layer, 3 units in the second hidden layer, and a single output.

(a) Draw a picture of the network.

(b) Write out an expression for f(X), assuming ReLU activation functions. Be as explicit as you can! (c) How many parameters are there?

2. Consider the Default data. Split the data into 70% training and 30% test.

```{r}
library
```

(a) Fit a neural network using a single hidden layer with 10 units.


(b) Compare the classification performance of your model with that of linear logistic regression.

3. In this problem, you will perform K-means clustering manually, with K = 2, on a small example. The observations are as follows.

```{r}
dat= data.frame(Ind = c(1,2,3,4,5,6),X1=c(1,1,0,5,6,4),X2=c(4,3,4,1,2,0))
dat
```

(a) Sketch the observations.

```{r}
plot(dat$X1,dat$X2, col = "red", pch = 19,xlab="X1", ylab="X2")
```

(b) Randomly assign a cluster label to each observation.

```{r}
set.seed(6)
dat$label <- rbinom(6, size = 1, prob = 0.5)
A0 =dat[(dat$label==0),]
B0 =dat[(dat$label==1),]
```

(c) Compute the centroid for each cluster.

```{r}
(centroid_A0 = colMeans(A0[2:3]))
(centroid_B0 = colMeans(B0[2:3]))

```

(d) Assign each observation to the centroid to which it is closest, in terms of Euclidean distance. Report the cluster labels for each observation.

```{r}
distance <- function(u, v) { 
  result <- numeric(nrow(u))
  for (i in 1:nrow(u)){
    result[i]=sqrt(sum((u[i,1] - v[1])^2)+sum((u[i,2] - v[2])^2)) }
    return(result)}
(dist_from_centroid_A0= distance(dat[2:3],centroid_A0)) 
(dist_from_centroid_B0= distance(dat[2:3],centroid_B0))

dat$color=ifelse(dist_from_centroid_A0<dist_from_centroid_B0,"red","darkblue")
dat
plot(dat$X1,dat$X2, col = dat$color, pch = 19)
points(centroid_A0[1],centroid_A0[2], col = "darkred" , pch = 19)
points(centroid_A0[1],centroid_A0[2], col = "red", pch = 5, cex = 2)
points(centroid_B0[1],centroid_B0[2], col = "blue" , pch = 19)
points(centroid_B0[1],centroid_B0[2],col = "blue", pch = 5, cex = 2)

```

red cluster points: (1,4),(1,3),(0,4)

blue cluster points: (5,1), (6,2),(4,0)

(e) Repeat (c) and (d) until the answers obtained stop changing.

(f) In your plot from (a), label the observations according to the final cluster labels obtained.

```{r}
# Clusters
A1 =dat[dat$color=="red",]
B1 =dat[(dat$color=="darkblue"),]

# Computing the 
(centroid_A1 = colMeans(A1[2:3]))
(centroid_B1 = colMeans(B1[2:3]))

(dist_from_centroid_A1= distance(dat[2:3],centroid_A1)) 
(dist_from_centroid_B1= distance(dat[2:3],centroid_B1))

dat$color=ifelse(dist_from_centroid_A1<dist_from_centroid_B1,"red","darkblue")
plot(dat$X1,dat$X2, col = dat$color, pch = 19)
points(centroid_A1[1],centroid_A1[2], col = "darkred" , pch = 19)
points(centroid_A1[1],centroid_A1[2], col = "red", pch = 5, cex = 2)
points(centroid_B1[1],centroid_B1[2], col = "blue" , pch = 19)
points(centroid_B1[1],centroid_B1[2],col = "blue", pch = 5, cex = 2)

A2 =dat[dat$color=="red",]
B2 =dat[(dat$color=="darkblue"),]

```



4. In this problem, you consider the gene expression data (Khan, in ISLR), and then perform clustering on the data.

```{r}
library(ISLR2)
gene.data=Khan
```


(a) Perform K-means clustering of the ”xtrain” with K = 4. How well do the clusters that you obtained in K-means clustering compare to the true class labels (”ytrain”)?

```{r}
xkm.gene = kmeans(gene.data$xtrain,4)
km.gene$cluster
gene.data$ytrain
table(km.gene$cluster,gene.data$ytrain)
(error.rate=(6+9+7+12+4+1+3)/63)
```

The error rate is very high (66.67 %),K-means clustering has a very poor performance.

(b) Using hierarchical clustering with complete linkage and Euclidean distance, cluster the states.
(c) Cut the dendrogram at a height that results in 4 distinct clusters.

```{r}
hc.gene.complete=hclust(dist(gene.data$xtrain),method = "complete")
plot(hc.gene.complete,main ="Complete Linkage",xlab= " Gene Expression")

# Cutting the dendrogram results in 4 clusters 4 
abline(h=57.6,col="red",lwd=1,lty=2)
```






